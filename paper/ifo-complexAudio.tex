\documentclass[paper-main.tex]{subfiles}

\begin{document}

In this section, we explore how this demonstration can be used to teach a selection of signal processing techniques. 
We use complex audio signals (such as music and speech) as a natural successor to the constant and wandering tones used in Sections~\ref{sec:single_tone} and~\ref{sec:viterbi_wandering} respectively.
As complex audio signals are not quasi-monochromatic, the Viterbi algorithm as applied in Section~\ref{sec:viterbi_wandering} is not directly applicable here. 
Instead we use a hierarchy of passive filters which suppress noise, yet make no assumption about the specific form of the signal, unlike the Fourier-based maximum likelihood filter which is tuned to the sinusoidal signals in Section~\ref{sec:single_tone} and Appendix~\ref{app:sinusoid_likelihood}.

Here we treat the Michelson interferometer as an optical microphone, where a laser beam is used to detect sound vibrations. 
Optical microphone devices have precedence in the laser microphones~\cite{laser_microphone} which are (or were historically) used in the defense industry and operate on a variety of related principles.
Our objective is to play a recording of speech or music through the speaker attached to mirror M1 (see Fig.XXXX), record the interference pattern, and then recover the original signal via a selection of signal processing techniques. 


This analysis is indirectly related to the advanced signal processing techniques required to extract gravitational-wave signals. 
Instead, the apparatus serves as an independent demonstration for a broader physics and engineering audience, particularly in undergraduate laboratories. 
We describe additional hardware components required for this demonstration in Section~\ref{sec:photodiode} and the initial results in Section~\ref{sec:initialResultsOpMic}. 
A selection of filters are applied to the data, details of which are presented in Supplementary Material and we present the results of the best performing filters in Section~\ref{sec:opticalMicResults}. \han{rephrase depending on what we show.}


\subsection{Hardware modifications for the optical microphone}
\label{sec:initialResultsOpMic}
The human ear can hear frequencies in the range of $\sim 20\,{\rm Hz}$--$20\,{\rm kHz}$. 
Speech intelligibility (being able to understand speech) requires frequencies up to $3\,{\rm kHz}$ and music requires up to and beyond $8\,{\rm kHz}$~\cite{speech_intelligibility}. Therefore, the optical microphone requires a sample rate of at least $16\,{\rm kHz}$ to capture both speech and music (adjusting for the Nyquist frequency). 
This cannot be achieved with the webcam used in Sections~\ref{sec:single_tone} and ~\ref{sec:viterbi_wandering} as it has a sampling rate of $30\,{\rm Hz}$ and thus can only ``hear'' frequencies below $15\,{\rm Hz}$.
To overcome this issue, we use a photodiode\footnote{A photodiode is an electrical component that acts as a regular diode when no light is incident on it, blocking any current flow in the reverse direction. As the intensity of incident light rises, it becomes increasingly conductive in the reverse direction.} at the output of the interferometer to achieve a sampling rate of $16\,{\rm kHz}$.

We place an OSRAM BPW21 photodiode in reverse-bias over an LM358 op-amp which together make a photo-detector that produces a voltage dependent on the incident intensity. 
The photodiode records the interference pattern at roughly the same off-centre position as the webcam in Sections~\ref{sec:single_tone} and~\ref{sec:viterbi_wandering}, again chosen arbitrarily.
The photodiode is mounted on a cloth screen re-purposed from the dismantled commercial speaker, with the electrical leads connected underneath the screen. 
The voltage signal from the photo-detector is captured by an MCP3008 $10$-bit analog-to-digital converter (ADC) connected to a Raspberry Pi Model 3 v1.2~\cite{RaspberryPi:online} (henceforth: Pi), which provides a convenient means to record the photodiode data.
Together, the circuit samples the signal at $\sim 16\,{\rm kHz}$. Resources for using the Pi and photodiode are described in Appendix~\ref{app:circuit_diagram}.

Sampling any frequency component of the analog signal above the Nyquist frequency of $8\,{\rm kHz}$ leads to aliasing\footnote{Folding of frequencies greater than half the sampling rate.} into the detected range. We include an anti-aliasing Sallen-Key filter~\cite{sallen_key_filter} tuned to $16\,{\rm kHz}$ before the ADC to prevent this from happening.
This component attenuates any frequencies above $8\,{\rm kHz}$, before they are digitally sampled. We also place a second cloth screen over the face of the photodiode to reduce the incident intensity and avoid saturating the ADC.
% It’s not clear what limited the sample rate to 16kHz, but it was likely non-optimal reading of the ADC by the Pi script.
% The ADC used (MCP3008) is quoted at 200kHz (or ksps, kilo samples per second).

%An updated schematic and a photograph of the entire optical microphone are shown in the left and right panels of Fig.~\ref{fig:ifo_schematic_podo}, respectively.
See Appendix~\ref{app:circuit_diagram} for a diagram and photograph of the circuit. 
\han{[Hannah] I've removed the second schematic and photo for now - could be an option for the photo to go in the appendix / supplement if we want to show the physical set up?}

\subsection{Anti-aliased output}
\label{sec:initialResultsOpMic}

%The source audio was played through the speaker with the optical microphone recording for the full duration of the audio (around a minute long for most sources). 
We test the optical microphone with a variety of recordings, including speech from different people and music ranging from simple melodies and rhythms to songs. During recordings, care is taken to minimize activity around the demonstration to reduce environmental noise coupling into the interferometer. 
The timeseries data is then directly converted to a .wav file and played as an audio recording using the \texttt{scipy.io.wavfile.write} function in Python~\cite{scipy,python}. 
When processing the results, we restrict our analysis to only the first $10\,{\rm s}$ of each observation (for efficiency), and only plot the first second here. 

The raw output of the optical microphone (with anti-aliasing) is noisy with a loud bass hum throughout. 
This can be explained by looking at the power spectral density (PSD) of the background noise (i.e., with the speaker switched off), as shown in the top panel of Fig.~\ref{fig:psd_noise}. 
The spectrum is dominated by AC mains noise with power from the fundamental $50\,{\rm Hz}$ mains signal up to and beyond $8$th harmonic thereof. 
The mains signal is also present (but far weaker) in the background spectrum taken with the photodiode in darkness, suggesting that ambient lighting has a large contribution. 
Besides lighting, other possible contributions to the mains signal include air conditioning and the photodiode circuit itself. 
The appearance of harmonics of the mains noise is likely due to the non-linearity discussed in Appendix~\ref{app:intensity_derivation} (see also Ref.~\cite{feynman}). 
The spectrum in Fig.~\ref{fig:psd_noise} also has a broad feature at around $750\,{\rm Hz}$, the origin of which is yet to be determined.


\begin{figure}
	\includegraphics[width=.49\textwidth]{figures/psd_butterworth_14_6.pdf}
	\caption{\label{fig:psd_noise}
\han{If we do not cover the butterworth filter in the main text, I'd suggest only showing the top panel of this figure in the main text. We can include the twp panel version in the butterworth section of the supplementary material}
Top: power spectral density (PSD) of background noise from the optical microphone (with the speaker off). 
Bottom: the PSD after applying a Butterworth bandpass filter (bottom panel) between the two frequencies marked with red, dashed lines. 
We see strong power from the $50\,{\rm Hz}$ mains hum and its harmonics (most likely from the photodiode circuit and the room’s lighting and cooling). Otherwise, the PSD is fairly white. 
After filtering we see strong attenuation (at least 3~dB) of all frequencies outside the band, but little change to the harmonics within the band.
}
\end{figure}


\subsection{Optical microphone results}
\label{sec:opticalMicResults}
\han{[Hannah] keen to hear other's thoughts, but I think it would be cool to show the notch+wiener combined and the logmmse as the results? They could be placed in a single figure for ease of comparison with time \& spectrum columns and rows: 1. original signal, 2. raw output, 3. combined cascaded notch and Wiener, 4. logMMSE}

We explore several filters to remove the $50\,{\rm Hz}$ mains hum and harmonics, and also to improve speech intelligibility of the recording. 
The accompanying supplementary material describes a range of analysis techniques which can be used as examples for the undergraduate laboratory. 
All filters are tested on the same $1\,{\rm s}$ long speech recording. 


In signal processing, the ideal filter would be one that:
i) completely attenuates the undesired parts of the spectrum, 
ii) does not change the rest of the spectrum, and 
iii) smoothly transitions between these regions, as to not damage the time domain signal when seen under convolution. 
However, these three conditions cannot all hold at once and any filter must compromise between them. \han{[Notes] editor's comments suggest expanding on this?}
Here we present results of two analysis.
They are briefly described and the results shown (see Supplementary Material for details). 

Firstly, we consider two signal processing techniques used in combination: the cascaded notch and Wiener filter. 
A notch filter remove signals within a specific frequency range \han{ref}. 
In our case, we would like to remove the $50\,{\rm Hz}$ mains noise and harmonics. 
Therefore, we use a cascaded notch filter where each notch is centred at each of the harmonics. 
The Wiener filter is an advanced statistical technique which makes used of statistical information from the speech data and noise. 
It amplifies parts of the signal with high SNR while suppressing pares with low SNR. 

\han{refer to figure.. } shows our results.
We find that the combined cascaded notch and Wiener filter produces a better result than either alone. 
Most of the mains noise is removed, however the recovered voice sounds muffled and is not understandable. 

Secondly, we apply a speech enhancement technique. 
Ref.~\cite{SubjectiveComparison} compares $13$ speech enhancement methods, finding the log minimum mean-square-error (logMMSE) estimator to be the best, qualitatively, at recovering speech (see also Ref.~\cite{Ephraim1984SpeechEU_logMMSE} for details on the logMMSE estimator). 
We use an existing implementation of the logMMSE~\cite{logmmse}.  


\han{refer to figure} 
After applying the logMMSE estimator we see significant attenuation of the mains harmonics and a general smoothing of the spectrum. 
Most of the background noise is removed, however the logMMSE does not significantly enhance the speech as the voice sounds muffled and indistinct.
We find some improvement with music over speech. 
Simple chords and drums can be heard, however more composite sounds and complex melodies cannot be heard clearly. 
Our observations suggest that this is especially true for certain instruments, in particular flutes and violins sometimes can’t be heard at all. 
This could be a perceptual effect or a frequency dependence somewhere in the optical microphone.
Speculating, perhaps the speaker-mirror coupling is stronger at low frequencies and thus instruments like electric bass and drums are louder in the results.
To address these problems, we need to determine whether the signals that are audibly missing (the diction in the speech and complex melodies in music) are indeed being transmitted through the optical microphone at all. 
To determine this requires a better understanding of the system, as discussed in Section~\ref{sec:future_work}.



\begin{comment} 
In this section, we explore how this demonstration can be used to teach a selection of signal processing techniques.
The apparatus used in Sections~\ref{sec:ifo} to~\ref{sec:viterbi_wandering} allows undergraduate students to explore a range of signal processing techniques and filters of interest and provides a foundation for many technical applications~\cite{DigitalProcingOfSpeechSignals:1978}. 
We provide a series of example activities using complex audio (such as music and speech) as a natural successor to constant and wandering tones. 
As complex audio signals are not quasi-monochromatic, unlike continuous gravitational-wave signals, the Viterbi algorithm as implemented in Section~\ref{sec:viterbi_wandering} is not directly applicable here. 
Instead, in this section, we use the interferometer as an optical microphone to detect and playback complex audio signals (i.e.\ using light to capture sound).
In doing so, we experiment with a hierarchy of passive filters which suppress noise, yet make no assumption about the specific form of the signal, unlike the Fourier-based maximum likelihood matched filter which is tuned to the sinusoidal signals in Section~\ref{sec:single_tone} and Appendix~\ref{app:sinusoid_likelihood}.
A vast literature exists on signal processing with speech, see Ref.~\cite{DigitalProcingOfSpeechSignals:1978} for introductory material including Fourier analysis and digital representations.
Optical microphone devices have precedence in the laser microphones~\cite{laser_microphone} which are (or were historically) used in the defence industry and operate on a variety of related principles.


This section is indirectly related to the advanced signal processing techniques required to extract gravitational-wave signals.
Here, the optical microphone serves as an independent demonstration for a broader physics and engineering audience, particularly in undergraduate laboratories.
We describe the additional hardware components required for this demonstration in Section~\ref{sec:photodiode} and initial results in Section~\ref{sec:initialResultsOpMic}. 
A selection of filter and speech enhancement example activities using the optical microphone are outlined in Section~\ref{sec:filters}.



\subsection{Hardware modifications for the optical microphone}
\label{sec:photodiode}


\begin{figure*}
	\includegraphics[width=0.44\textwidth]{figures/ifo_schematic_photodiode_edit.pdf}
	\includegraphics[width=0.52\textwidth]{figures/setup_pic2.pdf}
	\caption{
Left: the updated Michelson interferometer schematic. 
The schematic is identical to Figure~\ref{fig:interference_pattern} except that the data is now recorded using a photodiode and Raspberry Pi instead of a webcam. 
Right: a photograph of the optical microphone. 
The Michelson interferometer is shown on the left and the circuit and Raspberry Pi on the right. 
In the main image, the photodiode is placed behind the cloth screen. 
The inset shows a face-on view of the photodiode with the covering screen removed. 
See Appendix~\ref{app:circuit_diagram} for a circuit diagram and detailed photograph of the circuit.
}
	\label{fig:ifo_schematic_podo}
\end{figure*}



The human ear can hear frequencies in the range of $\sim 20\,{\rm Hz}$--$20\,{\rm kHz}$. Speech intelligibility (being able to understand speech) requires frequencies up to $3\,{\rm kHz}$ and music requires up to (and beyond) $8\,{\rm kHz}$~\cite{speech_intelligibility}. Therefore, the optical microphone requires a sample rate of at least $16\,{\rm kHz}$ to capture both speech and music (adjusting for the Nyquist frequency). This cannot be achieved with the webcam used in Sections~\ref{sec:single_tone} and ~\ref{sec:viterbi_wandering} as it has a sampling rate of $30\,{\rm Hz}$ and thus can only ``hear'' frequencies below $15\,{\rm Hz}$.
To overcome this issue, we use a photodiode\footnote{A photodiode is an electrical component that acts as a regular diode when no light is incident on it, blocking any current flow in the reverse direction. As the intensity of incident light rises, it becomes increasingly conductive in the reverse direction.} at the output of the interferometer to achieve a sampling rate of $16\,{\rm kHz}$.

We place an OSRAM BPW21 photodiode in reverse-bias over an LM358 op-amp which together make a photo-detector that produces a voltage dependent on the incident intensity. 
The photodiode records the interference pattern at roughly the same off-centre position as the webcam in Sections~\ref{sec:single_tone} and~\ref{sec:viterbi_wandering}, again chosen arbitrarily.
The photodiode is mounted on a cloth screen re-purposed from the dismantled commercial speaker, with the electrical leads connected underneath the screen. 
The voltage signal from the photo-detector is captured by an MCP3008 $10$-bit analog-to-digital converter (ADC) connected to a Raspberry Pi Model 3 v1.2~\cite{RaspberryPi:online} (henceforth: Pi), which provides a convenient means to record the photodiode data.
Together, the circuit samples the signal at $\sim 16\,{\rm kHz}$. Resources for using the Pi and photodiode are described in Appendix~\ref{app:circuit_diagram}.

Sampling any frequency component of the analog signal above the Nyquist frequency of $8\,{\rm kHz}$ leads to aliasing\footnote{Folding of frequencies greater than half the sampling rate.} into the detected range. We include an anti-aliasing Sallen-Key filter~\cite{sallen_key_filter} tuned to $16\,{\rm kHz}$ before the ADC to prevent this from happening.
This component attenuates any frequencies above $8\,{\rm kHz}$, before they are digitally sampled. We also place a second cloth screen over the face of the photodiode to reduce the incident intensity and avoid saturating the ADC.
% It’s not clear what limited the sample rate to 16kHz, but it was likely non-optimal reading of the ADC by the Pi script.
% The ADC used (MCP3008) is quoted at 200kHz (or ksps, kilo samples per second).

An updated schematic and a photograph of the entire optical microphone are shown in the left and right panels of Fig.~\ref{fig:ifo_schematic_podo}, respectively.
See Appendix~\ref{app:circuit_diagram} for a diagram and photograph of the circuit. 




\subsection{Anti-aliased output}
\label{sec:initialResultsOpMic}

%The source audio was played through the speaker with the optical microphone recording for the full duration of the audio (around a minute long for most sources). 
We test the optical microphone with a variety of recordings, including speech from different people and music ranging from simple melodies and rhythms to songs. During recordings, care is taken to minimise activity around the demonstration to reduce the amount of environmental noise coupling into the interferometer. The timeseries data is then directly converted to a .wav file and played as an audio recording using the \texttt{scipy.io.wavfile.write} function in Python~\cite{scipy,python}. When processing the results, we restrict our analysis to only the first $10\,{\rm s}$ of each observation (for efficiency), and only plot the first second here. 

The raw output of the optical microphone (with anti-aliasing) is noisy with a loud bass hum throughout. This can be explained by looking at the power spectral density (PSD) of the background noise (i.e., with the speaker switched off), as shown in the top panel of Fig.~\ref{fig:psd_noise}. The spectrum is dominated by AC mains noise with power from the fundamental $50\,{\rm Hz}$ mains signal up to and beyond $8$th harmonic thereof. The mains signal is also present (but far weaker) in the background spectrum taken with the photodiode in darkness, suggesting that ambient lighting has a large contribution. Besides lighting, other possible contributions to the mains signal include air conditioning and the photodiode circuit itself. The appearance of harmonics of the mains noise is likely due to the non-linearity discussed in Appendix~\ref{app:intensity_derivation} (see also Ref.~\cite{feynman}). The spectrum in Fig.~\ref{fig:psd_noise} also has a broad feature at around $750\,{\rm Hz}$, the origin of which is yet to be determined.


\begin{figure}
	\includegraphics[width=.49\textwidth]{figures/psd_butterworth_14_6.pdf}
	\caption{\label{fig:psd_noise}
Top: power spectral density (PSD) of background noise from the optical microphone (with the speaker off). 
Bottom: the PSD after applying a Butterworth bandpass filter (bottom panel) between the two frequencies marked with red, dashed lines. 
We see strong power from the $50\,{\rm Hz}$ mains hum and its harmonics (most likely from the photodiode circuit and the room’s lighting and cooling). Otherwise, the PSD is fairly white. 
After filtering we see strong attenuation (at least 3~dB) of all frequencies outside the band, but little change to the harmonics within the band.
}
\end{figure}


\subsection{A hierarchy of filters}
\label{sec:filters}

Here, we describe several filters used to remove the $50\,{\rm Hz}$ mains hum and its harmonics, but also to ultimately improve the speech intelligibility of the recording (to be able to understand the words spoken). 
We start with some na{\"i}ve approaches and then move to traditional signal processing filters (band-passing and cascaded notches). 
We finish by combining these with advanced statistical techniques (Wiener filter and the logMMSE estimator). 
As an illustrative example, we test all of these filters on the same $1\,{\rm s}$ long speech recording.

\begin{figure*}
	\begin{center}
    %% trim order is <left> <lower> <right> <upper>
    % height=0.5\textheight,trim={1cm 4.5cm 0.4cm 3.5cm},clip
	\includegraphics[width=.8\textwidth, trim={1cm 3cm 1cm 1cm}]{figures/notch_and_wiener_superplot_v2.pdf}
	\end{center}
	\caption{\label{fig:BackgroundNotchWienerCombined}
	Timeseries (left column) and frequency spectrum (right column) results for the background subtraction, notch, and Wiener filters.
    The source signal, shown in the first row, is a $1\,{\rm s}$ recording of an adult male voice (saying ``A cathode ...''). 
    The source signal is shifted by $0.12\,{\rm s}$ to the right to synchronise the manual delay from starting the recording with the Raspberry Pi and starting to play the source through the speaker. 
	The second row shows the raw output from the optical microphone when the input signal from the first row is played. 
    The third row shows a background noise recording from the optical microphone when no audio signal is played. 
    The fourth row shows the result of subtracting the background noise spectrum in the third row from the recording in the second row. 
	The fifth, sixth, and seventh rows show the results of applying the notch filter, the Wiener filter, and both combined, respectively.
	}
\end{figure*}


%\begin{figure*}
%\begin{center}
%% trim order is <left> <lower> <right> <upper>
%\includegraphics[width=\textwidth,trim={1cm 8.5cm 1cm 8cm},clip]{figures/freqSpectrumOriginalNoiseSubtracted.pdf}
%\end{center}
%\caption{\label{fig:noiseSubtract}
%Frequency spectrum of a $1\,{\rm s}$ signal before the use of filters. 
%The first panel shows the original frequency spectrum of the human voice recording (saying ``A cathode ...''). 
%The second panel shows the spectrum recorded after the signal passes through the optical microphone (sampled at $16\,000\,{\rm Hz}$).
%The third panel shows a noise-only recording from the optical microphone when no audio signal is being played. 
%The fourth panel shows the result of subtracting the noise-only spectrum in the third panel from the recording in the second panel. 
%We see no obvious improvement from this method. 
%}
%\end{figure*}

Results for a selection of filters are collated in Fig.~\ref{fig:BackgroundNotchWienerCombined} where the left and right columns show timeseries and Fourier spectrum results, respectively. 
The first and second rows of Fig.~\ref{fig:BackgroundNotchWienerCombined} show the source signal and the raw optical microphone recording, respectively. 
The third row shows a background spectrum of the optical microphone, as shown previously in the top panel Fig.~\ref{fig:psd_noise}.

%The Fourier spectrum of the source signal is shown in the first panel of Fig.~\ref{fig:BackgroundNotchWienerCombined}. The second panel shows the spectrum of the raw optical microphone recording of the signal. The third panel of Fig.~\ref{fig:noiseSubtract} shows a background spectrum of the optical microphone, as shown previously in the top panel Fig.~\ref{fig:psd_noise}.

Given that we have access to the background noise spectrum, an intuitive way to remove noise is via subtracting the noise spectrum from the recorded spectrum. 
The fourth row of Fig.~\ref{fig:BackgroundNotchWienerCombined} shows the spectrum obtained after subtracting the background noise spectrum. 
We see no obvious improvement, which may be attributed to a time-variant noise spectrum, the cause of which is unidentified. %Why the noise sources are time-varying is a fact yet to be determined.


Beyond this na{\"i}ve approach, we apply a range of filters to the data.
In signals processing, the ideal filter would be one that:
i) completely attenuates the undesired parts of the spectrum, 
ii) does not change the rest of the spectrum, and 
iii) smoothly transitions between these regions, as to not damage the time domain signal when seen under convolution. 
However, these three conditions cannot all hold at once and any filter must compromise between them, as we will see below. 
The following sections describe a series of example analyses with the optical microphone.


\subsubsection{Rectangular comb filter}

Simply zeroing the frequency bins corresponding to the harmonics of the $50\,{\rm Hz}$ mains signal is unsuccessful. This effectively multiplies the spectrum by a rectangular comb filter. It does remove the mains harmonics, but audibly ruins the rest of the signal due to lack of smoothness. This is because applying a filter in frequency space is equivalent to convolving the time-domain signal with the inverse Fourier transform of that filter. The inverse Fourier transform of a rectangular comb filter (a set of boxcars) is some combination of sinc functions, which significantly corrupt the signal. 
See also Section~\ref{sec:notch} where we explore a notch filter. 

%A smoother comb filter significantly attenuates the spectrum between each harmonic so that the signal is no longer audible. 
% \jam{[re:comb filters. When I tried to comb filter it either was too sharp and so jumbled the whole signal in convolution. Or if smooth attenuated the signal away into noise. At least, that's what I remember. Changrong has a comb filter section anyway so I am happy dropping my unsuccessful attempts.]}

\subsubsection{High-pass filter}

A high-pass filter smoothly attenuates frequencies below some cut-off frequency. Applying a high-pass filter, with a cut-off frequency around $150\,{\rm Hz}$, to the signal spectrum works well at removing the $50\,{\rm Hz}$ and $100\,{\rm Hz}$ harmonics. However, the mains harmonics above $100\,{\rm Hz}$ remain. Using a high-pass filter with a higher cut-off can be used to mitigate this issue. However, it makes the played-back signal unrecognisable as the region above $100\,{\rm Hz}$ carries a lot of the fundamental frequencies of speech and music~\cite{speech_intelligibility}.
Often in speech processing, the logarithm of the signal is taken since the amplitude information seems to be more important to intelligibility than the phase information to the human ear~\cite{SubjectiveComparison}. However, applying a high-pass filter to the logarithm of the signal spectrum does not significantly improve on the above simple high-pass filter.

%We also tried applying a high-pass filter to the logarithm of the signal spectrum and then exponentiating back, but this didn’t significantly improve on the above simple high-pass

\subsubsection{Butterworth band-pass filter}

\begin{figure}
	\includegraphics[width=0.49\textwidth]{figures/butterworth_150_3000.pdf}
	\caption{Butterworth bandpass filter frequency response. Any amplitude beyond $-3\,{\rm dB}$ is significant attenuation (half power). The red, dashed vertical lines show the band limits of $150\,{\rm Hz}$ and $3\,{\rm kHz}$. Note the flat response within the band characteristic of the Butterworth filter.}
	\label{fig:butterworth}
\end{figure}

A general band-pass filter combines a high-pass filter and a low-pass filter to smoothly attenuate frequencies outside of some band (alt.\ pass-band). A Butterworth band-pass filter is a particular band-pass filter such that the frequency response (the attenuation at each frequency) is ``maximally flat'' within the band. The Butterworth low-pass component is given by %Eqn.~\ref{eq:butterworth}

\begin{equation}
\label{eq:butterworth}
H(\omega) = \left[1+\varepsilon^2 \left( \frac{f}{f_c} \right)^{2n}\right]^{-1/2},
\end{equation}
% commenting to send to co-authors
%\jam{[Andrew wanted a quantitative comment about what the Butterworth does to the PSD, what do I use?]}
and is combined with a similar high-pass filter to form the band-pass filter.
In Eqn.~\ref{eq:butterworth}, $f_c$ is the cut-off frequency of the low-pass Butterworth filter, $\varepsilon$ is the gain, and $n$ is the order of the filter which determines how quickly the response rolls off past the cut-off frequency. Fig.~\ref{fig:butterworth} shows the frequency response of the filter used here, a fifth order ($n = 5$) Butterworth filter with a pass-band of $(150\,{\rm Hz}$, $3\,{\rm kHz})$. This high frequency cut-off is chosen since the frequencies important for speech generally lie below $2\,{\rm kHz}$~\cite{speech_intelligibility}.

The effect of applying this filter to the background noise PSD can be seen in the bottom panel of Fig.~\ref{fig:psd_noise}. The Butterworth band-pass filter reduces the amplitude of mains harmonics below $150\,{\rm Hz}$ and suppresses unrelated noise sources above $3\,{\rm kHz}$. However, it does not address the issue of mains harmonics above $150\,{\rm Hz}$ (i.e., in the pass-band). In the following section, we experiment with a cascade notch filter to address this.



\subsubsection{Cascade notch filter}
\label{sec:notch}
\begin{figure*}
\begin{center}
\includegraphics[width=.9\textwidth]{figures/notch_filter_response.jpg}
\end{center}
\caption{\label{fig:notchMagResponse}
Amplitude (magnitude) response for the first five notches of fifteen for the cascaded notch filter described by Eqs.~\ref{eqn:notch} and~\ref{eqn:notch15}. 
}
\end{figure*}

One method to remove mains noise and its harmonics is to use a sequence of notch filters centred at each of the frequency bins we want to remove. This sequence is known as a ``cascade'' of notches. Here, the notches are smooth in comparison to the na{\"i}ve zeroing of each frequency (which looks like a rectangular comb). The complex frequency response of a typical infinite impulse response (IIR) notch filter can be written as \citep{10.5555/541204}
\begin{equation}
    \label{eqn:notch}
    H(z)=\frac{1+\alpha}{2}\frac{1-2\beta z^{-1}+z^{-2}}{1-\beta(1+\alpha)z^{-1}+\alpha z^{-2}}\,,
\end{equation}
where $z$ is the complex frequency and $\alpha$ and $\beta$ are parameters that control the filter. $w_0=\cos^{-1}(\beta)$ is the frequency that is completely attenuated (zeroed or ``notched'') at the centre of the notch and $B_w=\cos^{-1}[2\alpha/(1+\alpha^2)]$ is the bandwidth of the notch (which determines how quickly the response changes around the notched frequency).
We find that a sequence of $15$ notches works well here, with the $k^\mathrm{th}$ notch centred on the $k^\mathrm{th}$ harmonic of the $50\,{\rm Hz}$ mains signal, where $k=0,2,\dots,14$. When then choose the bandwidth and order (here equal to six) of each notch to avoid disturbing useful signals while still allowing for uncertainty in the location of each harmonic of the mains signal. The response of this cascaded notch filter $H(z)$ is the product of the responses of each of the individual $H_k(z)$ notches,
\begin{equation}
    \label{eqn:notch15}
    H(z) = \prod_{k=0}^{14} H_k(z).
\end{equation}

We use the built-in MATLAB filter design toolbox in this work~\cite{MATLAB}. 
The amplitude (magnitude) response of the first five notch filters is shown in Fig.~\ref{fig:notchMagResponse}. 
The time series and spectrum obtained after applying the cascade notch filter to the speech recording are shown in the fifth row of Fig.~\ref{fig:BackgroundNotchWienerCombined}.
We see that the mains harmonics are significantly attenuated in comparison background subtracted results shown in the fourth row of Fig.~\ref{fig:BackgroundNotchWienerCombined}.

Although the notch filter removes much of the mains hum sound, the filtered recording is not intelligible. Qualitatively it sounds more like a drum than a human voice. This is due to loss of voice information under the filter.
To overcome this, we turn to more advanced techniques, starting with the Wiener filter. Instead of just passively filtering different frequencies, this statistical technique optimises an estimate of the original signal. 




% \begin{figure*}
% \begin{center}
% \includegraphics[width=\textwidth,trim={1cm 8.2cm 1 8.3cm},clip]{figures/time SeriesNotchWiener.pdf}
% \end{center}
% \caption{\label{fig:timeNotchWiener}
% Time series results for the notch and Wiener filters. 
% The first panel shows the original speech recording which was played from the speaker. 
% The second panel shows the data from the optical microphone before any filters are applied. 
% The third and fourth panels show results after applying the notch and Wiener filter respectively. 
% Finally, the fifth panel shows the result of applying both the notch and Wiener filter. 
% }
% \end{figure*}

% \begin{figure*}
% \begin{center}
% \includegraphics[width=\textwidth,trim={0cm 8.2cm 0cm 8cm},clip]{figures/freqNotchWiener.pdf}
% \end{center}
% \caption{\label{fig:freqNotchWiener}
% Frequency spectrum results for the notch and Wiener filters. 
% The first and second panels show the results of applying the notch and Wiener filter respectively.
% The third panel shows the spectrum after applying both the notch and Wiener filters. 
% The first, second, and third panels show the spectrum of the time series show in the third, fourth, and fifth panels of Fig.~\ref{fig:timeNotchWiener} respectively. 
% }
% \end{figure*}

%A notch filter attenuates signals in specific frequency bands. 
%We apply a $15$ cascaded notched filter to remove the mains noise and its harmonics up to $750\,{\rm Hz}$. 
%\han{should we include here a mathematical description of the $15$ cascade notch filter?}
%\jam{[Yes, I have no idea what a cascaded notched filter is and I'm an author!]}



\subsubsection{Wiener filter}
\label{sec:Wiener}

A Wiener filter is an advanced statistical technique that estimates the injected signal given prior information about the injected spectrum and the reference spectrum of the background noise. 
The observed noisy speech signal sequence is given as $\mathbf{x}=(x(0),\dots, x(N-1))$, where $N$ is the length of the data sequence and $\mathbf{x}$ is the sum of the original injected signal $\mathbf{s}=(s(0),\dots,s(N-1))$, and the noise sequence $\mathbf{w}=(w(0),\dots,w(N-1))$, 
\begin{equation}
    \mathbf{x}=\mathbf{s}+\mathbf{w}\,.
\end{equation}
Given $\textbf{x}$, our goal is to make an estimate $\hat{\textbf{s}}$ of the original signal $\textbf{s}$ such that we minimise the Bayesian mean-square-error (BMSE) between the two, defined as 
\begin{equation}
\label{eq:BMSE}
\text{BMSE}(\hat{\textbf{s}})=E[(\textbf{s}-\hat{\textbf{s}})^2]\,.
\end{equation}
If we assume that: i) $\textbf{x}$ is ``wide sense stationary''~\footnote{ A random process $\{x(t)\}$ is wide sense stationary if, for all $t_1,t_2 \in R$, (1) its mean is time invariant, i.e., $\mu_x(t_1)=\mu_x(t_2)=\text{constant}$; and (2) the autocorrelation depends only on the time difference, i.e., $R_x(t_1,t_2)=R_x(\tau),\tau=t_1-t_2$.} with zero mean; ii) the signal $\textbf{s}$ has a mean of zero; and iii) the noise $\textbf{w}$ is uncorrelated with the signal $\textbf{s}$, we can further express $\hat{\textbf{s}}$ to be a linear combination of present and past observed data
\begin{equation}
\hat{{s}}[n]=\sum_{k=0}^{n}h[k]x[n-k]\,,
\end{equation}
where $\textbf{h}=(h(0),\dots h(n))$ represents the coefficients of an $n$th order Wiener filter.
% What this sentence is trying to convey is not clear to me
%% just trying to say that we can make the assumptions that we do because the Wiener filter appears to work well under them - J
%These assumptions are to be justified by the efficacy of the Wiener filter.
%Given the observation data $\mathbf{x}$, our goal is to estimate $\mathbf{s}$ so that the error between $\mathbf{x}$ and the estimate ${\hat{\mathbf{s}}}$ is minimized in the mean square sense,
%\begin{equation}
%    \hat{\mathbf{s}}=\mathop{\arg\min}_{\textbf{s}}||\mathbf{x}-\mathbf{s}||_2^2\,. 
%\end{equation}
%Assuming that $\textbf{x}$ is wide-sense stationary~\footnote{
%A random process $\{x(t)\}$ is wide sense stationary (WSS) if for all
%$t_1,t_2 \in R$, it satisfies (1) mean is time invariant, i.e.,
%$\mu_x(t_1)=\mu_x(t_2)=\text{constant}$; (2) autocorrelation depends
%only on time difference, i.e., $R_x(t_1,t_2)=R_x(\tau),\tau=t_1-t_2$.}
%and that $\textbf{w}$ is stationary and uncorrelated with the signal, we represent the $n$th order Wiener filter by its coefficients $\textbf{h}=\{h(0),\dots(n)\}$. 
The famous Wiener-Hopf equation~\citep{noble1959methods} allows us to determine $\textbf{h}$, as\begin{equation}
\label{eqn:wiener-hopf}
\begin{bmatrix}  
r_{xx}[0]&r_{xx}[1]&\dots& r_{xx}[n]\\
r_{xx}[1]&r_{xx}[0]&\dots &r_{xx}[n-1]\\
\vdots&\vdots&\ddots&\vdots\\
r_{xx}[n]&r_{xx}[n-1]&\dots &r_{xx}[0]
\end{bmatrix}
\begin{bmatrix}
h[0]\\
h[1]\\
\vdots\\
h[n]
\end{bmatrix}=
\begin{bmatrix}
r_{ss}[0]\\
r_{ss}[1]\\
\vdots\\
r_{ss}[n]
\end{bmatrix}\,,
\end{equation}
where $r_{xx}$ and $r_{ss}$ are the auto-correlation functions of $\mathbf{x}$ and $\mathbf{s}$ between timestep $i$ and $i+n$, 
%\begin{equation*}
%\left\{ 
\begin{eqnarray} 
r_{xx}[n] &~=~& E[x(i)~x(i+n)] \,,\\
r_{ss}[n] &~=~& E[s(i)~s(i+n)] \,.
%r_{xx}[n] &=& E[x(i)\, &x(i+n)] \\ 
%r_{ss}[n] &=& E[s(i)\, &s(i+n)] 
\end{eqnarray} 
%\right\}  
%\end{equation*}
%where $r_{xx}$ and $r_{ss}$ is the auto-correlation function of $x$ and $s$.


%As an instructive aside, i
If we let the Wiener filter be non-causal (i.e. we estimate the current signal based on both past \emph{and future} observations), then we can represent Eqn.~\ref{eqn:wiener-hopf} in the frequency domain as
\begin{equation}
\label{eqn:wiener}
    H(f)=\frac{P_{ss}(f)}{P_{xx}(f)}=\frac{P_{ss}(f)}{P_{ss}(f)+P_{ww}(f)}\,,
\end{equation}
where $P_{xx}(f), P_{ss}(f), P_{ww}(f)$ are the spectra of the observed noisy data, the injected signal, and the background noise, respectively. Intuitively, from Eqn.~\ref{eqn:wiener}, we can see that the non-causal Wiener filter amplifies the input signal where the signal to noise ratio (SNR) is high and attenuates the signal where the SNR is low. The causal Wiener filter (that only makes estimates based on past observations) is similar. More detailed analysis of both kinds of Wiener filters can be found in Ref.~\citep{10.5555/151045}.
% That this filter is non-causal makes it unsuitable for real time signal processing 

%In this work, we construct a causal Wiener filter based on Eqn.~\ref{eqn:wiener-hopf}, and set the length of the filter to $n=100$. 
%A larger $n$ provides more smoothing of the input signal, however requires more computational memory. 
%The choice of $n=100$ is a reasonable selection to balance smoothing and efficiency. 

In this work, we construct a higher-order causal Wiener filter based on Eqn.~\ref{eqn:wiener-hopf}. A higher-order Wiener filter provides greater smoothing of the input signal but also increases the computational memory required. For this work, we choose a Wiener filter of order $n=100$ as it provides a reasonable balance between smoothing and efficiency. The timeseries and frequency spectrum after applying the Wiener filter are shown in the sixth row of Fig.~\ref{fig:BackgroundNotchWienerCombined}.
We see a significant improvement in the timeseries of the recovered signal, however, a strong noise hum persists, audibly. 
%The Wiener filter produces an estimated signal $\hat{x}$ using linear time-invariant filtering. 
%It minimizes the mean square error $\mathbb{E}[(\hat{x}-x)^2]$ between $\hat{x}$ and the actual signal $x$~\citep{verhoeven2009robust}.
%\han{should we add more details here too?} \jam{[seconded]}
%The time series and frequency spectrum results of the Wiener filter are shown in the fourth panel of Fig.~\ref{fig:timeNotchWiener} and the second panel of Fig.~\ref{fig:freqNotchWiener} respectively. 
%The time series is visibly cleaner, however there is a strong noise hum. 


\subsubsection{Combined notch and Wiener filter}

Here, we experiment with applying a combination of the cascaded notch and the Wiener filter to the recorded speech signal. 


The Wiener filter makes use of statistical information from the speech data and noise. It amplifies the part of the signal with high SNR while suppressing the parts with low SNR (see above Sec.~\ref{sec:Wiener}). It is implemented in the form of a finite response filter, which ensures linear phase response and stability (both desirable), but at the cost of high orders computationally. By comparison, the notch filter is based on directly removing the unwanted frequency components. It is implemented in the form of an infinite impulse response filter. Although it significantly decreases the order of the overall filter, it unavoidably introduces nonlinear phase and instability. 


By combining the notch and Wiener filter, we can trade-off between the two and achieve an overall better performance, as can be seen in the bottom row of Fig.~\ref{fig:BackgroundNotchWienerCombined}.
The filtered voice after the combined notch and Wiener filter is enhanced compared to either alone. The mains noise is all but removed and more voice information is retained. However, the recovered voice still sounds muffled and is not understandable.


%The combination of the notch filter and the Wiener filter produces a cleaner signal than either individually. 
%The time series and spectrum results are shown in the final panels of Figs.~\ref{fig:timeNotchWiener} and ~\ref{fig:freqNotchWiener} respectively.

%The ideal filter would be one that i) completely attenuates the undesired parts of the spectrum, ii) does not change the rest of the spectrum, and iii) has smooth edges so as to not damage the signal after convolution. 
%These three conditions are contradictory and any filter must compromise between them. 
%The above simple filters fail one or more of i)--iii). 
%The Butterworth filter performs the best as it is optimised to be `maximally flat' within the band region, prioritising the second condition.
%\han{see note from Andrew and ask James about this}




%\han{Is it worth also including some plots to show how the different filters perform? E.g. a PSD with comb filter, high pass filter, Butterworth filter?}

% Note that we also replicated the Viterbi algorithm results from before with the photodiode, but only with the mains hum removed and high-passing above 100Hz, else the algorithm never selected the signal. Except for being over audible frequencies, there isn’t much difference in the tracking once these filters are applied.

% I just combined these into a single figure and also moved some of the intro text about the signal into fig 8 so that it is clear we are all talking about the same data in these plots - hannah
\begin{figure*}
	%\includegraphics[width=0.49\textwidth]{figures/combined_timeseries_melatos_shift_source.pdf}
	%\includegraphics[width=0.49\textwidth]{figures/combined_spectrum_melatos.pdf}
    \includegraphics[height=0.57\textheight]{figures/combined_timeseries_melatos_shift_source_tall.pdf}
    \includegraphics[height=0.57\textheight]{figures/combined_spectrum_melatos_tall.pdf}
    \caption{\label{fig:logMMSE_timeseries_freqspectrum}
Timeseries (left column) and frequency spectrum (right columns) results with the logMMSE estimator. 
The original source signal and optical microphone recording in the top and middle panels respectively are for comparison and are identical to those shown in the top two panels in Fig.~\ref{fig:BackgroundNotchWienerCombined} (again, the source signal is shifted by $0.12\,{\rm s}$). 
The bottom-left panel shows the timeseries of the recording after filtering with the logMMSE estimator, where the rise at the start of the timeseries is expected when filtering a signal of finite duration.
The bottom-right panel shows the corresponding frequency spectrum with the logMMSE estimator. 
The spectrum shows only a detail of the total frequency domain (there is little activity at higher frequencies) and has truncated peaks at amplitude $2$ which otherwise dominate the plot. 
The change in scale from the top panel is not of concern as the ear hears the relative frequency content and normalisation of the timeseries fixes any scaling. 
% my feeling is not to include this sentence here as it is hard to be certain what the noise and what is the input signal for this plot without further investigation - hannah
%The bottom two panels also show a broad feature from around $750\,{\rm Hz}$ to $1200\,{\rm Hz}$, the origin of which is yet to be determined.
    }
\end{figure*}


\subsubsection{logMMSE estimator}
\label{sec:logmmse}




\begin{comment}
\begin{figure*}
	\includegraphics[width=0.8\textwidth]{figures/combined_timeseries_melatos_shift_source.pdf}
	\caption{Time series of an adult male voice (saying ``A cathode ...'') showing the source (top panel), the optical microphone recording (middle panel), and the recording after filtering with the logMMSE estimator (bottom panel). The source signal is shifted by $0.12$ s to the right to synchronise the manual delay from starting to play the source through the speaker and starting the recording at the Raspberry Pi. One second of the one minute long recording is shown. The rise at the start of the bottom panel is expected when filtering a signal of finite duration.}
	\label{fig:logMMSE_timeseries}
\end{figure*}

\begin{figure*}
	\includegraphics[width=0.8\textwidth]{figures/combined_spectrum_melatos.pdf}
	\caption{Spectrum of optical microphone recording of the same adult male voice in Fig.~\ref{fig:logMMSE_timeseries} showing the source (top panel), the optical microphone recording (middle panel), and the recording after filtering with the logMMSE estimator (bottom panel). The spectrum shows only a detail of the total frequency domain (there is little activity at higher frequencies) and has truncated peaks at amplitude 2 which otherwise dominate the plot. The change in scale from the top panel is not of concern as the ear hears the relative frequency content and normalisation of the time series fixes any scaling. The bottom two panels also show a broad feature from around $750\,{\rm Hz}$ to $1200\,{\rm Hz}$, the origin of which is yet to be determined.}
	\label{fig:logMMSE_spectrum}
\end{figure*}


%probably a bit too causal language 
%What we’re attempting here is not new, 
Speech enhancement of noisy channels is a classic problem in signal processing. 
% shouldn't we make the citations consistent throughout? We don't mention author names up to this point and typically journals have a set style guide. - Hannah 
%To borrow from that field, Yi Hu and P.C. Loizon~\cite{SubjectiveComparison} compared $13$ speech enhancement methods and found the log minimum means-square error (logMMSE) estimator to be the best among them, qualitatively, at recovering speech. 
In Ref.~\cite{SubjectiveComparison}, a comparison is made of $13$ speech enhancement methods, finding the log minimum mean-square-error (logMMSE) estimator to be the best, qualitatively, at recovering speech. 
This estimator is based on speech enhancement techniques discussed in Ref.~\cite{Ephraim1984SpeechEU_logMMSE} and minimises the mean-square-error (MSE) of the estimate from the injected signal, like the Wiener filter above, except that is measures the MSE between the logarithm of the Fourier amplitudes. This is motivated by the fact that the logarithm approximates the response of the human ear~\cite{SubjectiveComparison}. We apply an existing implementation of the logMMSE estimator (see Ref.~\cite{logmmse}) to the recorded signal.
%Hu~\&~Loizou~(2006)~\cite{SubjectiveComparison} compare 13 speech enhancement methods and find the best to be a log minimum mean-square error (log-MMSE) estimator. 
%This type of estimator is based on work Ephraim~\&~Malah~(1984)~\cite{Ephraim1984SpeechEU_logMMSE} into speech enhancement by minimising the mean square error (MSE) to the logarithm of the spectral (Fourier) amplitude.

%In this work we use an existing implementation~\cite{logmmse} of the logMMSE estimator. 
%Applying this estimator to the optical microphone output produced dramatically cleaner results. 
 



The timeseries and frequency spectrum results of the logMMSE estimator are shown in Fig.~\ref{fig:logMMSE_timeseries_freqspectrum}. 
The top two rows show the original speech signal and anti-aliased reading from the optical microphone for ease of comparison (identical to the top two rows in Fig.~\ref{fig:BackgroundNotchWienerCombined}). 
The third row shows the timeseries and corresponding spectral amplitude after applying the logMMSE estimator. 
We see significant attenuation of the mains harmonics as well as a general smoothing of the spectrum. 
The logMMSE method indeed removes most of the background noise but does not significantly enhance the speech as the voice sounds muffled and indistinct.
% suggest remove as it's just rephrasing what's already been said
%, with the words unable to be understood.


We find some improvement with music over speech. After applying the logMMSE estimator, simple chords and drums can be heard, however more composite sounds and complex melodies cannot be heard clearly. 
%Our observations suggest that this is especially true for instruments of certain timbres (harmonic profiles), in particular flutes and violins sometimes can’t be heard at all. 
Our observations suggest that this is especially true for certain instruments, in particular flutes and violins sometimes can’t be heard at all. 
This could be a perceptual effect or a frequency dependence somewhere in the optical microphone.
%, although the exact cause is yet to be determined.  - repetition
Speculating, perhaps the speaker-mirror coupling is stronger at low frequencies and thus instruments like electric bass and drums are louder in the results.
To address these problems, we need to determine whether the signals that are audibly missing (the diction in the speech and complex melodies in music) are indeed being transmitted through the optical microphone at all. 
%Or if instead they are present in the noisy data but we need better techniques to extract them. - should probably justify what you mean by better if you are going to say it here, but I think enough is discussed in section VI for it not to be necessary here - hannah 
To determine this requires a better understanding of the system, as discussed in Section~\ref{sec:future_work}.



%Overall, we find that the intelligibility of speech remains low. 
%The played-back voice sounds mumbled and indistinct. 
%The estimator removes most of the background noise, but does not clarify the speech. 
%To address these problems with the recordings we need to determine whether the signals that are audibly missing (the diction in the speech and complex melodies in music) are indeed being transmitted through the optical microphone at all. 
%This requires a better understanding of the system.
% The fringe counting problem of the detector, wherein multiple fringes passing over the array in a single speaker deflection artificially raises the frequency, was noticeable in the speech recordings when compared to the source audio. However, upon comparing injected pure tones, the increase appears to be minimal and inconsistent, at around 5Hz maximum difference. An simple explanation is that the speaker deflections are small compared to manual pressing on the optical table.

\end{comment}

\end{document}
