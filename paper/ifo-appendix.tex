\documentclass[paper-main.tex]{subfiles}

\begin{document}



\section{Open-source code}
\label{app:code}
This project is implemented in Python 3 scripts and jupyter notebooks and MATLAB. 
We refer the reader to the Supplementary Material for software references. 
The current build and sample data can be found at:
\url{https://github.com/daccordeon/gravexplain}







\section{Detecting a sinusoidal signal in Gaussian noise}
\label{app:sinusoid_likelihood}


In this appendix, we demonstrate that the modulus of the Fourier transform is an appropriate detection statistic when searching for a sinusoidal signal in Gaussian noise. 
We describe the data as
\begin{equation}
x(t) = s(t) + n(t)\,, 
\label{eqn:GNdata}
\end{equation}
where $s(t)$ and $n(t)$ are the signal and noise, respectively.
The signal takes the form
\begin{equation}
s(t) = A \cos\left[{\omega t + \phi}\right]\,,
\label{eqn:GNmodel}
\end{equation}
where $A$, $\omega$, and $\phi$ are the amplitude, angular frequency and phase of the signal, respectively. 
The noise $n(t)$ is a fluctuating zero-mean time series with the following property: if we define an inner product between two arbitrary time series $u(t)$ and $v(t)$ as 
\begin{equation}
\ip{u}{v} = \frac{1}{T} \int_0^T \mathrm{d} t \, u(t) v(t)\,,
\label{eqn:ipuv}
\end{equation}
where $T$ is the total time of the observation, then the likelihood $\mathcal{L}$ of measuring the noise-noise product $\ip{n}{n}$ is given by 
\begin{equation}
\mathcal{L} = \exp\left( -\frac{1}{2}\ip{n}{n}\right)\,.
\label{eqn:ipnn}
\end{equation}
Equations~\ref{eqn:ipuv} and~\ref{eqn:ipnn} define what it means for noise to be Gaussian through the fundamental measurement of $\ip{n}{n}$.





The likelihood of measuring the signal $s(t)$ in the presence of noise follows from Eqns.~\ref{eqn:GNdata} and~\ref{eqn:ipnn} by replacing $n(t)$ in Eqn.~\ref{eqn:ipnn} with $x(t) - s(t) = n(t)$ from Eqn.~\ref{eqn:GNdata}.~\cite{JKS:1998,Jaynes:2003}
The result is
\begin{eqnarray}
\mathcal{L} &=& \exp\left( -\frac{1}{2}\ip{x-s}{x-s} \right) \label{eqn:likeOne}\,, \\ 
            &=& \exp\left( -\frac{1}{2}\ip{x}{x} - \frac{A^2}{2} \ip{\cos[\omega t + \phi]}{\cos[\omega t + \phi]} \right. \nonumber\\
                 &&\hspace{3em} \left. +  A \ip{x}{\cos[\omega t + \phi]} \vphantom{\frac12}\right) \,,\\ 
                 &=& \exp \left( -\frac{1}{2}\ip{x}{x} - \frac{A^2}{4} + A \ip{x}{\cos[\omega t + \phi]} \right)\,.\label{eqn:logLike}
\end{eqnarray}
We then maximise Eqn~.\ref{eqn:logLike} with respect to $A$, obtaining 
\begin{eqnarray}
\label{eqn:maxLogLike}
\mathcal{L}_\mathrm{max} = \exp \left( - \frac{1}{2} \ip{x}{x} + \ip{x}{\cos[\omega t + \phi]}^2 \right)\,,
\end{eqnarray}
for $A = 2\ip{x}{\cos[\omega t + \phi]}$.
From the second term of Eqn.~\ref{eqn:maxLogLike}, we see that the maximum likelihood of a sinusoidal signal in Gaussian noise is the modulus of the cosine Fourier transform, plus the term $\ip{x}{x}$, which is independent of $\omega$ and $\phi$ and can therefore be ignored when searching over $\omega$.

Two important points must be made about the above procedure. 
(i) Fundamentally the goal is to maximize $\mathcal{L}$ in Eqn.~\ref{eqn:likeOne} by varying $s(t)$ through $A$.
For the special case of the Gaussian likelihood (Eqn.~\ref{eqn:ipnn}), which peaks at $\ip{n}{n}= 0$, this is equivalent to minimizing the difference between $x(t)$ and $s(t)$ as evident in Eqn.~\ref{eqn:ipuv}.
In general, however, minimizing the difference between $x(t)$ and $s(t)$ is not equivalent always to the fundamental goal of maximising $\mathcal{L}$, for example if $\mathcal{L}$ peaks at $\ip{n}{n} \neq 0$, or if $\mathcal{L}$ has multiple maxima.
(ii) The maximum likelihood $\mathcal{L}_{\rm max}$ in Eqn.~\ref{eqn:maxLogLike} (or equivalently its logarithm) defines the detection statistic. 
When its value exceeds a threshold (chosen freely by the analyst) at some value of $\omega$, a signal is deemed to have been detected at that value of $\omega$.
Therefore the specific functional form of Eqn.~\ref{eqn:maxLogLike} matters, which is a second reason why one must start from Eqn.~\ref{eqn:ipnn} rather than Eqn.~\ref{eqn:likeOne}, in addition to reason (i).




\section{Viterbi algorithm}
\label{app:viterbi}
This appendix contains some details regarding the implementation of the Viterbi algorithm described in Section~\ref{sec:viterbi}. 
The Viterbi algorithm~\cite{Viterbi:1967} is a classic method in signal processing, whose theoretical underpinnings and implementation are accessible to undergraduate students. 
See the Supplementary Material for further resources and other pseudocode examples available online.

Here, we present some pseudocode (below) of the implementation used in Section~\ref{sec:viterbi}. We use Fourier amplitudes, normalized between $(0, 1)$ by dividing by the maximum value in the grid, as multiplicative weights. 
To avoid numerical underflow we take the logarithm of the Fourier amplitudes, which we can equivalently use as additive weights.


Let $X$ be a grid of $j=0,\ldots,N_f$ rows and $i=0,\ldots,N_t$ columns of additive weights for each node. Let Y and Z be grids of the same shape to store the weight of the best path to each node and the row index of the previous node on that path, respectively. Let W be a length $N_t+1$ array to store the final sequence of row indices for the optimal path. We restrict paths to only move one cell up or down at a time (or stay constant). For the boundary cases of $j=0,N_f$, we only search over $k \in \{0,1\}$ and $k \in \{-1,0\}$, respectively, to stay inside the grid (this is not shown in the pseudocode below).

\begin{algorithmic}
\Function{Viterbi}{$X$}
    % \State $a \gets a+1$
    % \State \Return $a$

    \For{each row $j=0,\ldots,N_f$}
		\State $Y_{0,j} \gets X_{0,j}$
    \EndFor

    \For{each column $i=1,\ldots,N_t$}
	    \For{each row $j=0,\ldots,N_f$}
		
	    	\State $Y_{i,j} \gets X_{i,j} + \underset{k \in \{-1,0,1\}}{\max} (Y_{i-1,j+k})$
	    	\State $Z_{i,j} \gets j + \underset{k \in \{-1,0,1\}}{\arg\max} (Y_{i-1,j+k})$
   
	    \EndFor
    \EndFor

    \State $W_{N_t} \gets \underset{j=0,\ldots,N_f}{\arg\max} (Y_{N_t,j})$

    \For{each col $i=N_t-1,\ldots,0$}

		\State $W_i \gets Z_{i+1, W_{i+1}}$

    \EndFor    

    \State \Return $W$
\EndFunction
\end{algorithmic}



\end{document}
